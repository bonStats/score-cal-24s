---
title: "Bayesian score calibration for approximate models"
author: "Joshua J Bon"
institute: "CEREMADE, UniversitÃ© Paris-Dauphine"
bibliography: refs.bib
format:
  revealjs:
    html-math-method: mathjax 
    theme: default
    df-print: paged
    incremental: true 
    css: style.css
editor: visual
---

```{r setup, load_refs, include=FALSE, cache=FALSE}
library(ggplot2)
library(dplyr)
library(tidyr)
library(scales)
library(RColorBrewer)
library(kableExtra)
library(purrr)
library(distributional)
library(ggdist)

source("figs/illustrative-plots.R")

```

## Talk overview

-   Motivation and intuitions
-   Theory and method
-   Examples

. . .

Joint work with

-   **Christopher Drovandi**, QUT
-   **David Warne**, QUT
-   **David Nott**, NUS

## Motivation: "Little rascal" likelihoods

::: columns
::: {.column width="70%"}
Current methods for **intractable** likelihoods:

-   10,000s of realisations of the data generating process (ABC, pseudo-marginal MCMC)
-   May not target the true posterior (ABC)
-   How to identify method failure?
:::

::: {.column width="30%"}
![](imgs/little_rascals_our_gang_waiting.gif)
:::
:::

## But we have approximations... {.center}

## Approximations everywhere

**Likelihood approximations**

-   Limiting distributions
-   Whittle likelihood
-   Model simplification (e.g. SDE -\> ODE)
-   Linear noise approximation

## Approximations everywhere

**Posterior approximations**

-   Laplace
-   Variational inference
-   Approximate Bayesian Computation (ABC)

## Approximations everywhere {center="true"}

**Sampling approximations**

-   Importance sampling
-   Sequential Monte Carlo
-   Markov chain Monte Carlo (MH, ULA, MALA, HMC)

## And we can learn functions... {.center}

## Learning functions

-   Represent posterior approximation with Monte Carlo samples
    -   e.g. MCMC with approximate likelihood
-   Find a function to correct the approximate samples posterior
    -   No need to rerun MCMC
        -   unlike correcting the likelihood directly
    -   Do approximate inference once
        -   in parallel for multiple datasets

## Illustration {.center}

## Correcting a posterior

```{r}
#| label: fig-illustration-1
#| warning: false
#| echo: false
#| fig-cap: 'Ideal case (approximation in blue)'

sim_id <- 1

ggplot() +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id), method = "mvnorm", fill = "purple") +
  geom_point(aes(x,y), obs_data %>% filter(sim %in% sim_id)) +
  #geom_point(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), alpha = 0.2) +

  geom_hdr(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), method = "mvnorm", fill = "blue") + #, probs = 0.95) +
  geom_point(aes(x,y), priortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  geom_point(aes(x,y), posteriortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  facet_grid( ~ val, scales = "fixed") +
  theme_bw() + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())

```

## Correcting a posterior

```{r}
#| label: fig-illustration-2
#| warning: false
#| echo: false
#| fig-cap: 'Real case (approximation in blue)'

sim_id <- 1

ggplot() +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id, val == "Prior"), method = "mvnorm", fill = "purple") +
  geom_point(aes(x,y), obs_data %>% filter(sim %in% sim_id)) +
  #geom_point(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), alpha = 0.2) +
  geom_hdr(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), method = "mvnorm", fill = "blue") + #, probs = 0.95) +
  geom_point(aes(x,y), priortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  geom_point(aes(x,y), posteriortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  facet_grid( ~ val, scales = "fixed") +
  theme_bw() + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())

```

## Correcting many posteriors

```{r}
#| label: fig-illustration-3
#| warning: false
#| echo: false
#| fig-cap: 'Correct on average? (approximation in blue)'

sim_id <- 2:4

ggplot() +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id, val == "Prior"), method = "mvnorm", fill = "purple") +
  geom_point(aes(x,y), obs_data %>% filter(sim %in% sim_id)) +
  #geom_point(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), alpha = 0.2) +
  geom_hdr(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), method = "mvnorm", fill="blue") + #, probs = 0.95) +
  geom_point(aes(x,y), priortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  geom_point(aes(x,y), posteriortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  facet_grid(sim ~ val, scales = "fixed") +
  theme_bw() + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())

```

## Measure the similarity

$$\mathbb{E}[S({\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta})]$$

$$\tilde{y} \sim P(\cdot \vert {\color{red}\theta}), \quad {\color{red}\theta} \sim {\color{purple}\Pi}$$

-   ${\color{blue}K}(\cdot\vert \tilde{y})$ is the approximate posterior
-   ${\color{red}\theta}$ is the true data generating parameter
-   $({\color{red}\theta}, \tilde{y})$ prior-predictive pairs
-   $S$ measures the similarity between ${\color{blue}K}(\cdot\vert \tilde{y})$ and point ${\color{red}\theta}$

## Optimise the similarity

$$\mathbb{E}[S({\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta})]$$

## Optimise the similarity

$$\mathbb{E}[S({{\color{green}f}_\sharp\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta})]$$

## Optimise the similarity

$$\max_{f \in \mathcal{F}}\mathbb{E}[S({\color{green}f}_\sharp{\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta})]$$


## Optimise the similarity

$$\max_{f \in \mathcal{F}}\mathbb{E}[S({\color{green}f}_\sharp{\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta})]$$

$$\tilde{y} \sim P(\cdot \vert {\color{red}\theta}), \quad {\color{red}\theta} \sim {\color{purple}\Pi}$$

-   ${\color{green}f}$ transforms realisations of ${\color{blue}K}(\cdot\vert \tilde{y})$

## Correcting many posteriors

```{r}
#| label: fig-illustration-4
#| warning: false
#| echo: false
#| fig-cap: 'Before correction'

sim_id <- 5:7

ggplot() +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id, val == "Prior"), method = "mvnorm", fill = "purple") +
  geom_point(aes(x,y), obs_data %>% filter(sim %in% sim_id)) +
  #geom_point(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), alpha = 0.2) +
  geom_hdr(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), method = "mvnorm", fill="blue") + #, probs = 0.95) +
  geom_point(aes(x,y), priortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  geom_point(aes(x,y), posteriortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  facet_grid(sim ~ val, scales = "fixed") +
  theme_bw() + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())

```

## Correcting many posteriors

```{r}
#| label: fig-illustration-5
#| warning: false
#| echo: false
#| fig-cap: 'After correction'

sim_id <- 5:7

ggplot() +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id, val == "Prior"), method = "mvnorm", fill = "purple") +
  geom_hdr(aes(x,y), distributions %>% filter(sim %in% sim_id, val == "Posterior"), method = "mvnorm", fill = "blue") +
  geom_point(aes(x,y), obs_data %>% filter(sim %in% sim_id)) +
  #geom_point(aes(x,y), approx_posterior_data %>% filter(sim %in% sim_id), alpha = 0.2) +
  geom_point(aes(x,y), priortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  geom_point(aes(x,y), posteriortrue_data %>% filter(sim %in% sim_id), colour = "red") +
  facet_grid(sim ~ val, scales = "fixed") +
  theme_bw() + theme(legend.position = "none", axis.title.x=element_blank(), axis.title.y=element_blank())

```

## Model calibration by simulation

1.  Correct coverage? *Frequentist*
    -   $S({\color{green}f}_\sharp{\color{blue}K}(\cdot\vert y), {\color{red}\theta}) = \vert 1({\color{red}\theta} \in {\color{purple}\text{CR}_\alpha}) - \alpha \vert$
2.  Can we correct the entire distribution? **Bayesian**
    -   Proper scoring rules

# Theory and method

## Scoring rules

<br>

$S(U,x)$ compares probabilistic *forecast* $U$ to *ground truth* $x$.

$$S(U,V) = \mathbb{E}_{x\sim V} S(U,x)$$

where $V$ is a probability measure.

## Proper scoring rules

[@gneiting2007strictly]

<br>

. . .

$S(U, \cdot)$ is **strictly proper** if

-   $S(V,V) \geq S(U, V)$ for all $V$ in some family, and
-   equality holds iff $U = V$.

. . .

$$V = \arg\max_{U \in \mathcal{U}} S(U, V)$$

## What we can't do

<br>

$$\Pi(\cdot ~\vert~y ) = \underset{{\color{green}f}\in \mathcal{F}}{\arg\max}~ S[{\color{green}f}_\sharp {\color{blue}K}(\cdot\vert y),\Pi(\cdot ~\vert~y )]$$

## What we can't do

<br>

$$\Pi(\cdot ~\vert~y ) = \underset{{\color{green}f} \in \mathcal{F}}{\arg\max}~ \mathbb{E}_{\theta \sim \Pi(\cdot ~\vert~y )}\left[S({\color{green}f}_\sharp{\color{blue}K}(\cdot\vert y),\theta) \right]$$

## What we can do

<br>

Consider the joint data-parameter space instead

. . .

$$\Pi(\text{d}\theta ~\vert~\tilde{y} )P(\text{d}\tilde{y}) = P(\tilde{y}~\vert~\theta)\Pi(\text{d}\theta )$$

. . .

See @pacchiardi2022likelihood and @bon2022bayesian

## What we can do

<br>

$$\Pi(\cdot ~\vert~y ) = \underset{{\color{green}f} \in \mathcal{F}}{\arg\max}~ \mathbb{E}_{\tilde{y} \sim P} \mathbb{E}_{\theta \sim \Pi(\cdot ~\vert~\tilde{y} )}\left[S({\color{green}f}_\sharp {\color{blue}K}(\cdot\vert \tilde{y}),\theta) \right]$$

## What we can do

<br>

$$\Pi(\cdot ~\vert~y ) = \underset{{\color{green}f} \in \mathcal{F}}{\arg\max}~ \mathbb{E}_{{\color{red}\theta} \sim {\color{purple}\Pi}} \mathbb{E}_{\tilde{y} \sim P(\cdot ~\vert~{\color{red}\theta})}\left[S({\color{green}f}_\sharp {\color{blue}K}(\cdot\vert \tilde{y}),{\color{red}\theta}) \right]$$

## Two steps further

<br>

1.  Replace $P(\text{d}\tilde{y})$ with $Q(\text{d}\tilde{y}) \propto P(\text{d}\tilde{y})v(\tilde{y})$

2.  Replace $\Pi$ with $\bar{\Pi}$

    -   correct with importance weight

. . .

It still works!

## Supporting theory

-   Strictly proper scoring rule $S$ w.r.t. $\mathcal{P}$

-   Importance distribution $\bar\Pi$ on $(\Theta,\vartheta)$

    -   $\Pi \ll \bar\Pi$ with density $\bar\pi$

-   Stability function $v:\mathsf{Y} \rightarrow [0,\infty)$

    -   measurable under $P$ on $(\mathsf{Y}, \mathcal{Y})$

-   $Q(\text{d} \tilde{y}) \propto P(\text{d} \tilde{y})v(\tilde{y})$

-   Family of kernels $\mathcal{K}$

## Supporting theory

**Theorem 1**:

If $\mathcal{K}$ is *sufficiently rich* then the Markov kernel,

$$\bbox[5pt,border: 1px solid blue]{\color{black}K^{\star} \equiv \underset{K \in \mathcal{K}}{\arg\max}~\mathbb{E}_{\theta \sim \bar\Pi} \mathbb{E}_{\tilde{y} \sim P(\cdot ~\vert~ \theta)}\left[w(\theta, \tilde{y}) S(K(\cdot ~\vert~ \tilde{y}),\theta) \right]}$$

where $w(\theta, \tilde{y}) = \frac{\pi(\theta)}{\bar\pi(\theta)} v(\tilde{y})$ then

$$\bbox[5pt,border: 1px solid blue]{K^{\star}(\cdot ~\vert~ \tilde{y}) = \Pi(\cdot ~\vert~ \tilde{y})}$$ almost surely.

## Bayesian Score Calibration

![](figs/score-cal-schem.png)

## Moment-correcting transformation

$$f(x) = L[x - \hat{\mu}(y)] + \hat{\mu}(y) + b$$

-   Mean $\hat{\mu}(y) = \mathbb{E}(\hat{\theta}),\quad \hat{\theta} \sim \hat\Pi(\cdot~\vert~y)$ for $y \in \mathsf{Y}$.

-   $L$ is a lower triangular matrix with positive elements on diagonal.

## What about the weights?

$$w(\theta, \tilde{y}) = \frac{\pi(\theta)}{\bar\pi(\theta)} v(\tilde{y})$$

Be brave...

. . .

![](imgs/dumb_patrick.gif)

## What about the weights?

<br>

**Unit weights**

$$\hat{w}(\theta, \tilde{y}) = 1$$

. . .

Justified asymptotically using the flexibility of $v(\tilde{y})$

# Examples

## OU Process

$$\text{d}X_t = \gamma (\mu - X_t) \text{d}t + \sigma\text{d}W_t$$

. . .

Observe final observation at time $T$ ( $n= 100$):

. . .

$$X_T \sim \mathcal{N}\left(  \mu + (x_0 - \mu)e^{-\gamma T}, \frac{D}{\gamma}(1- e^{-2\gamma T}) \right)$$

. . .

where $D = \frac{\sigma^2}{2}$. Fix $\gamma = 2$, $T=1$, $x_0 = 10$

## OU Process (limiting approximation)

Infer $\mu$ and $D$ with **approximate likelihood** based on

$$X_\infty  \sim \mathcal{N}\left(\mu, \frac{D}{\gamma}\right)$$

## OU Process (limiting approximation)

{{< pdf figs/ou-process-plot-uni-1.pdf height=460 width=1000 >}}

$M = 100$ and $\bar\Pi = \hat\Pi(\cdot~\vert~y)$ with variance scaled by 2

## OU Process (limiting approximation)

Comparison for $\mu$

```{r, ou-mu-table-1, echo = F}
mu_dat_ou1 <- tibble::tribble(
  ~`Posterior`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage (90%)`,
      "Approx", 1.54, 1.21, 0.22, 0,
      "Adjust (Î±=0)", 0.12, 0.15, 0.20, 64,
      "Adjust (Î±=0.5)", 0.12, 0.15, 0.23, 81,
      "Adjust (Î±=1)", 0.12, 0.15, 0.23, 82,
      "True", 0.12, -0.01, 0.26, 94
  )
knitr::kable(mu_dat_ou1, format = "html")
```

Estimated from independent replications of the method.

## OU Process (limiting approximation)

Comparison for $D$

```{r, ou-D-table-1, echo = F}
D_dat_ou1 <- tibble::tribble(
  ~`Posterior`, ~`MSE`, ~`Bias`, ~`St. Dev`, ~`Coverage (90%)`,
      "Approx", 4.73, 0.18, 1.46, 85,
      "Adjust (Î±=0)", 4.83, 0.28, 1.24, 72,
      "Adjust (Î±=0.5)", 5.08, 0.41, 1.42, 81,
      "Adjust (Î±=1)", 5.13, 0.42, 1.45, 83,
      "True", 5.00, 0.37, 1.48, 85
  )
knitr::kable(D_dat_ou1, format = "html")
```

Estimated from independent replications of the method.

## Lotka-Volterra SDE (quasi-ABC)

[Turing.jl Tutorial: Bayesian Estimation of Differential Equations](https://turing.ml/dev/tutorials/10-bayesian-differential-equations/#inference-of-a-stochastic-differential-equation)

$$\text{d} X_{t} = (\beta_1 X_{t} - \beta_2 X_{t} Y_{t} ) \text{d} t+ \sigma_1 \text{d} B_{t}^{1}$$ $$\text{d} Y_{t} = (\beta_4 X_{t} Y_{t} - \beta_3 Y_{t} ) \text{d} t+ \sigma_2 \text{d} B_{t}^{2}$$

## Lotka-Volterra SDE (quasi-ABC)

Use noisy quasi-likelihood as **approximate likelihood**

$$\begin{aligned}&L(\beta_{1:4}, \tau ~\vert~ x_{1:n}, y_{1:n}) \\
&= \tau^{2n}\exp\left(-\frac{\tau^2}{2} \sum_{i=1}^{n}\left[(x_i^\prime - x_i)^{2} + (y_i^\prime - y_i)^{2}\right] \right)\end{aligned}$$

-   $\{(x_i^\prime,y_i^\prime)\}_{i=1}^{n}$ simulated conditional on $\beta_{1:4}$
-   Prior $\tau â¼ \text{Gamma}(2, 3)$

. . .

**Further approximation**: use rough numerical integration scheme (Euler-Maruyama $\Delta t=0.01$).

## Lotka-Volterra SDE (quasi-ABC)

{{< pdf figs/lotka-posterior-comparison.pdf height=600 width=800 >}}

## Lotka-Volterra SDE (quasi-ABC)

{{< pdf figs/lotka-calcheck.pdf height=600 width=900 >}}

## Lotka-Volterra SDE (EKF)

Use extended Kalman Filter as **approximate likelihood**

- Discretise SDE at time points $t_0,t_1,\ldots,t_n$
- Assume Gaussian at each $t_i$
- Approximate transition dynamics by linear assumption

## Lotka-Volterra SDE (EKF)

{{< pdf figs/lotka-ekf-calcheck.pdf height=600 width=900 >}}

## Thanks for listening

Ongoing work: happy to talk

-   Collaborators: Chris, David$^2$
-   Helpful commentators

## Contact

-   [joshuajbon\@gmail.com](mailto:joshuajbon@gmail.com)

-   [twitter/bonStats](https://twitter.com/bonStats)

## References

::: {#refs}
:::

# Appendix: Theory vs practice

## Justification of unit weights

When we take $\alpha =1$, $\hat{w} = 1$!

. . .

**Theorem 2**

Let $g(x) = \bar \pi(x) / \pi(x)$ positive and continuous for $x \in \Theta$.

If an estimator $\theta^{\ast}_n \equiv \theta^{\ast}(\tilde{y}_{1:n})$ exists such that $\theta^{\ast}_n \rightarrow z$ a.s. for $n \rightarrow \infty$ when $\tilde{y}_i \sim P(\cdot ~\vert~ z)$ for $z \in \Theta$ then the error when using $\hat{w} = 1$ satisfies $$\hat{w} - w(\theta,\tilde{y}_{1:n}) \rightarrow 0$$ a.s. for $n \rightarrow \infty$

## Justification of unit weights

Theorem 2 is possible because of the stability function justified by Theorem 1.

$$w(\theta, \tilde{y}) = \frac{\pi(\theta)}{\bar\pi(\theta)} v(\tilde{y})$$

. . .

**Trick**: set $v(\tilde{y}) = \frac{\bar\pi(\theta^{\ast}_n )}{\pi(\theta^{\ast}_n )}$ and look at large sample properties.

. . .

Don't need to explicitly know the estimator $\theta^{\ast}_n$!

## Sufficiently rich kernel family

-   $\mathcal{K}$ be a family of Markov kernels
-   $\mathcal{P}$ be a class of probability measures
-   $Q$ be a probability measure on $(\mathsf{Y},\mathcal{Y})$, and $\tilde{y}\sim Q$
-   $\Pi(\cdot ~\vert~ \tilde{y})$ be the true posterior at $\tilde{y}$.

. . .

We say $\mathcal{K}$ is sufficiently rich with respect to $(Q,\mathcal{P})$ if for all $U \in \mathcal{K}$, $U(\cdot ~\vert~ \tilde{y}) \in \mathcal{P}$ almost surely and there exists $U \in \mathcal{K}$ such that $U(\cdot ~\vert~ \tilde{y}) = \Pi(\cdot ~\vert~ \tilde{y})$ almost surely.

## Sufficiently rich kernel families

-   The moment-correcting transformation is typically not sufficiently rich
-   Unit weights give the stabilising function the following property

. . .

$v(\tilde{y}_{1:n}) \rightarrow \delta_{\hat{\theta}_0}(\tilde{\theta}^\ast_n)$ for $n\rightarrow\infty$ where

-   $\hat{\theta}_0$ is the MLE for the true data $y$
-   $\tilde{\theta}^\ast_n$ is the MLE for simulated data $\tilde{y}$

## Sufficiently rich kernel families

$v(\tilde{y}_{1:n}) \rightarrow \delta_{\hat{\theta}_0}(\tilde{\theta}^\ast_n)$

Recall the distribution is defined as

$$
Q(\text{d}\tilde{y}) \propto P(\text{d}\tilde{y}) v(\tilde{y})
$$

-   we are concentrating on simulated datasets consistent with $\hat{\theta}_0$
-   simulated datasets with the same sufficient statistics
-   approximate posteriors $\hat\Pi(\cdot~\vert~\tilde{y})$ will be equal
-   global bias and variance correction terms are sufficiently rich

## Sufficiently rich kernel families

Results are *targeted* towards $\bar\Pi$

-   finite sample approximation using importance sampling

-   manipulation of the weights (unit weights)

-   Trade-off between:

    -   insufficiency of moment-correcting transformation + approximate posterior
    -   targeting high-probability regions of $\bar\Pi$

# Appendix: Another example

## Bivariate OU Process (VI)

$$\text{d}X_t = \gamma (\mu - X_t) \text{d}t + \sigma\text{d}W_t$$ $$\text{d}Y_t = \gamma (\mu - Y_t) \text{d}t + \sigma\text{d}W_t$$ $$Z_t = \rho X_t + (1-\rho)Y_t$$

Model $(X_t,Z_t)$ with setup as in the univariate case, $(x_0,z_0)=(5,5)$ and use a mean-field **variational approximation**.

## Bivariate OU Process (VI)

{{< pdf figs/gen-corr-ou-process-plot-contour.pdf height=600 width=800 >}}

## Bivariate OU Process (VI)

**Correlation summaries**

```{r, corr-table, echo = FALSE}
corr_tab <- tribble(
  ~Posterior, ~Mean, ~`St. Dev.`,
"Approx", 0.00, 0.02,
"Adjust (Î±=0)", 0.18, 0.41,
"Adjust (Î±=0.5)", 0.37, 0.16,
"Adjust (Î±=1)", 0.37, 0.15,
"True", 0.42, 0.06
)
knitr::kable(corr_tab, format = "html")
```

$M = 100$ and $\bar\Pi = \hat\Pi(\cdot~\vert~y)$ with variance scaled by 2

## Bivariate OU Process (VI)

{{< pdf figs/gen-corr-ou-process-plot-calcheck.pdf height=600 width=650 >}}
